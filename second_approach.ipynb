{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import amrlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# ::snt This is a test of the system.\n",
      "(t / test-01\n",
      "      :ARG1 (s / system)\n",
      "      :domain (t2 / this))\n",
      "# ::snt This is a second sentence.\n",
      "(s / sentence\n",
      "      :ord (o / ordinal-entity\n",
      "            :value 2)\n",
      "      :domain (t / this))\n"
     ]
    }
   ],
   "source": [
    "stog = amrlib.load_stog_model()\n",
    "graphs = stog.parse_sents(['This is a test of the system.', 'This is a second sentence.'])\n",
    "for graph in graphs:\n",
    "    print(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m gtos \u001b[38;5;241m=\u001b[39m \u001b[43mamrlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_gtos_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m sents, _ \u001b[38;5;241m=\u001b[39m gtos\u001b[38;5;241m.\u001b[39mgenerate(graphs)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sents:\n",
      "File \u001b[1;32mc:\\Users\\stoic\\miniconda3\\envs\\llms_env\\lib\\site-packages\\amrlib\\__init__.py:43\u001b[0m, in \u001b[0;36mload_gtos_model\u001b[1;34m(model_dir, **kwargs)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m gtos_model\n\u001b[0;32m     42\u001b[0m model_dir \u001b[38;5;241m=\u001b[39m model_dir \u001b[38;5;28;01mif\u001b[39;00m model_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(defaults\u001b[38;5;241m.\u001b[39mdata_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_gtos\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 43\u001b[0m gtos_model \u001b[38;5;241m=\u001b[39m load_inference_model(model_dir, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m gtos_model\n",
      "File \u001b[1;32mc:\\Users\\stoic\\miniconda3\\envs\\llms_env\\lib\\site-packages\\amrlib\\models\\model_factory.py:75\u001b[0m, in \u001b[0;36mload_inference_model\u001b[1;34m(model_directory, **kwargs)\u001b[0m\n\u001b[0;32m     73\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m meta\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkwargs\u001b[39m\u001b[38;5;124m'\u001b[39m, {})   \u001b[38;5;66;03m# get any model kwargs from the meta-data\u001b[39;00m\n\u001b[0;32m     74\u001b[0m model_kwargs\u001b[38;5;241m.\u001b[39mupdate(kwargs)             \u001b[38;5;66;03m# override them with amything passed in\u001b[39;00m\n\u001b[1;32m---> 75\u001b[0m model \u001b[38;5;241m=\u001b[39m model_class(model_directory, meta[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_fn\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32mc:\\Users\\stoic\\miniconda3\\envs\\llms_env\\lib\\site-packages\\amrlib\\models\\generate_xfm\\inference.py:25\u001b[0m, in \u001b[0;36mInference.__init__\u001b[1;34m(self, model_dir, model_fn, model, tokenizer, **kwargs)\u001b[0m\n\u001b[0;32m     23\u001b[0m default_device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     24\u001b[0m device \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m, default_device)\n\u001b[1;32m---> 25\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForSeq2SeqLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_dir\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mtask_specific_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtranslation_amr_to_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Load the tokenizer. If kwargs passes in tok_name_or_path use that otherwise get from the config\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\stoic\\miniconda3\\envs\\llms_env\\lib\\site-packages\\transformers\\modeling_utils.py:3110\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3105\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[0;32m   3106\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   3107\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3108\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3109\u001b[0m         )\n\u001b[1;32m-> 3110\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\stoic\\miniconda3\\envs\\llms_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1340\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1337\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1338\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m-> 1340\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\stoic\\miniconda3\\envs\\llms_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\stoic\\miniconda3\\envs\\llms_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[1;31m[... skipping similar frames: Module._apply at line 900 (4 times)]\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\stoic\\miniconda3\\envs\\llms_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\stoic\\miniconda3\\envs\\llms_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:927\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    923\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    924\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    925\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    926\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 927\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    928\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    930\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\stoic\\miniconda3\\envs\\llms_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1326\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m   1320\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[0;32m   1321\u001b[0m             device,\n\u001b[0;32m   1322\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1323\u001b[0m             non_blocking,\n\u001b[0;32m   1324\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[0;32m   1325\u001b[0m         )\n\u001b[1;32m-> 1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1330\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1331\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gtos = amrlib.load_gtos_model()\n",
    "sents, _ = gtos.generate(graphs)\n",
    "for sent in sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# ::snt This is a test of the SpaCy extension.\n",
      "(t / test-01\n",
      "      :ARG1 (e / extend-01\n",
      "            :ARG1 (p / product\n",
      "                  :name (n / name\n",
      "                        :op1 \"SpaCy\")))\n",
      "      :domain (t2 / this))\n",
      "# ::snt The test has multiple sentences.\n",
      "(h / have-03\n",
      "      :ARG0 (t / test)\n",
      "      :ARG1 (s / sentence\n",
      "            :quant (m / multiple)))\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "amrlib.setup_spacy_extension()\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp('This is a test of the SpaCy extension. The test has multiple sentences.')\n",
    "graphs = doc._.to_amr()\n",
    "for graph in graphs:\n",
    "    print(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import amrlib\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "amrlib.setup_spacy_extension()\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your JSONL file\n",
    "file_path = \"data/massive_amr.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the JSONL file\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    data = [json.loads(line) for line in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract sentences and AMR graphs\n",
    "sentences = [entry[\"utt\"] for entry in data]\n",
    "amr_graphs = [entry[\"raw_amr\"] for entry in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### STOG: Sentences to AMR ###\n",
      "\n",
      "Sentence: what are some updates about the stock market\n",
      "AMR Graph:\n",
      "# ::snt what are some updates about the stock market\n",
      "(u / update-02\n",
      "      :ARG1 (m / market\n",
      "            :mod (s / stock))\n",
      "      :ARG2 (a / amr-unknown)\n",
      "      :quant (s2 / some))\n",
      "\n",
      "Sentence: definition of velocity\n",
      "AMR Graph:\n",
      "# ::snt definition of velocity\n",
      "(d / define-01\n",
      "      :ARG1 (v / velocity))\n",
      "\n",
      "Sentence: please look up exchange between us and mexico\n",
      "AMR Graph:\n",
      "# ::snt please look up exchange between us and mexico\n",
      "(l / look-up-05\n",
      "      :polite +\n",
      "      :mode imperative\n",
      "      :ARG0 (y / you)\n",
      "      :ARG1 (e / exchange-01\n",
      "            :ARG0 (w / we)\n",
      "            :ARG2 (c / country\n",
      "                  :name (n / name\n",
      "                        :op1 \"Mexico\"))))\n",
      "\n",
      "Sentence: can you describe to me what a pineapple looks like\n",
      "AMR Graph:\n",
      "# ::snt can you describe to me what a pineapple looks like\n",
      "(p / possible-01\n",
      "      :polarity (a / amr-unknown)\n",
      "      :ARG1 (d / describe-01\n",
      "            :ARG0 (y / you)\n",
      "            :ARG1 (l / look-02\n",
      "                  :ARG0 (p2 / pineapple)\n",
      "                  :ARG1 (t / thing))\n",
      "            :ARG2 (ii / i)))\n",
      "\n",
      "Sentence: what is the dollar against the pound\n",
      "AMR Graph:\n",
      "# ::snt what is the dollar against the pound\n",
      "(d / dollar\n",
      "      :prep-against (p / pound)\n",
      "      :domain (a / amr-unknown))\n"
     ]
    }
   ],
   "source": [
    "# --- Test STOG: Convert Sentences to AMR ---\n",
    "print(\"\\n### STOG: Sentences to AMR ###\")\n",
    "parsed_graphs = stog.parse_sents(sentences[:5])  # Test on first 5 sentences\n",
    "for sent, graph in zip(sentences[:5], parsed_graphs):\n",
    "    print(f\"\\nSentence: {sent}\\nAMR Graph:\\n{graph}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### GTOS: AMR to Sentences ###\n",
      "\n",
      "AMR:\n",
      "(u / update-02\n",
      "      :ARG2 (a / amr-unknown)\n",
      "      :topic (m / market-01\n",
      "            :ARG1 (s / stock))\n",
      "      :mod (s2 / some))\n",
      "Reconstructed Sentence: What are some stock market updates?\n",
      "\n",
      "AMR:\n",
      "(d / define-01\n",
      "      :ARG1 (v / velocity)\n",
      "      :ARG2 (a / amr-unknown))\n",
      "Reconstructed Sentence: What is the definition of velocity?\n",
      "\n",
      "AMR:\n",
      "(l / look-up-05 :mode imperative :polite +\n",
      "      :ARG0 (y / you)\n",
      "      :ARG1 (e / exchange-01\n",
      "            :ARG1 (c / currency\n",
      "                  :mod (c3 / country :name (n / name :op1 \"us\")))\n",
      "            :ARG3 (c2 / currency\n",
      "                  :mod (c4 / country :name (n2 / name :op1 \"mexico\")))))\n",
      "Reconstructed Sentence: Please look up exchange rates between US and Mexican currency.\n",
      "\n",
      "AMR:\n",
      "(d / describe-01 :mode imperative :polite +\n",
      "      :ARG0 (y / you)\n",
      "      :ARG1 (t / thing\n",
      "            :ARG1-of (l / look-02\n",
      "                  :ARG0 (f / food-dish :name (n / name :op1 \"pineapple\")))))\n",
      "Reconstructed Sentence: Please describe how the pineapple looks.\n",
      "\n",
      "AMR:\n",
      "(h / have-quant-91\n",
      "      :ARG1 (c / currency :name (n / name :op1 \"dollar\"))\n",
      "      :ARG2 (a / amr-unknown)\n",
      "      :ARG4 (c2 / currency :name (n2 / name :op1 \"pound\")))\n",
      "Reconstructed Sentence: How many are dollars compared to pounds?\n"
     ]
    }
   ],
   "source": [
    "# --- Test GTOS: Convert AMR to Sentences ---\n",
    "print(\"\\n### GTOS: AMR to Sentences ###\")\n",
    "reconstructed_sentences, _ = gtos.generate(amr_graphs[:5])  # Test on first 5 AMRs\n",
    "for amr, recon_sent in zip(amr_graphs[:5], reconstructed_sentences):\n",
    "    print(f\"\\nAMR:\\n{amr}\\nReconstructed Sentence: {recon_sent}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### SpaCy AMR Extension ###\n",
      "\n",
      "Sentence: what are some updates about the stock market\n",
      "SpaCy AMR Graph:\n",
      "# ::snt what are some updates about the stock market\n",
      "(u / update-02\n",
      "      :ARG1 (m / market\n",
      "            :mod (s / stock))\n",
      "      :ARG2 (a / amr-unknown)\n",
      "      :quant (s2 / some))\n",
      "\n",
      "Sentence: definition of velocity\n",
      "SpaCy AMR Graph:\n",
      "# ::snt definition of velocity\n",
      "(d / define-01\n",
      "      :ARG1 (v / velocity))\n",
      "\n",
      "Sentence: please look up exchange between us and mexico\n",
      "SpaCy AMR Graph:\n",
      "# ::snt please look up exchange between us and mexico\n",
      "(l / look-up-05\n",
      "      :polite +\n",
      "      :mode imperative\n",
      "      :ARG0 (y / you)\n",
      "      :ARG1 (e / exchange-01\n",
      "            :ARG0 (w / we)\n",
      "            :ARG2 (c / country\n",
      "                  :name (n / name\n",
      "                        :op1 \"Mexico\"))))\n"
     ]
    }
   ],
   "source": [
    "# --- Test SpaCy + AMR ---\n",
    "print(\"\\n### SpaCy AMR Extension ###\")\n",
    "for sent in sentences[:3]:  # Test on first 3 sentences\n",
    "    doc = nlp(sent)\n",
    "    doc_graphs = doc._.to_amr()\n",
    "    for graph in doc_graphs:\n",
    "        print(f\"\\nSentence: {sent}\\nSpaCy AMR Graph:\\n{graph}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's try to translate the dataset to Irish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from transformers import MarianMTModel, MarianTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 1. Load Translation Model (English → Irish)\n",
    "# ----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Helsinki-NLP/opus-mt-en-ga\"  # English to Irish\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 2. Load JSONL Data\n",
    "# ----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"data/massive_amr.jsonl\"\n",
    "output_file = \"data/massive_amr_irish.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 3. Translate Function\n",
    "# ----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_text(text, tokenizer, model):\n",
    "    \"\"\"Translates English text to Irish using MarianMT.\"\"\"\n",
    "    if not text.strip():\n",
    "        return text  # Skip empty strings\n",
    "\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "    translated_tokens = model.generate(**inputs)\n",
    "    translated_text = tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]\n",
    "\n",
    "    return translated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 4. Translate Sentences\n",
    "# ----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entry in data:\n",
    "    entry[\"utt\"] = translate_text(entry[\"utt\"], tokenizer, model)\n",
    "    entry[\"annot_utt\"] = translate_text(entry[\"annot_utt\"], tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 5. Save Translated Data\n",
    "# ----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for entry in data:\n",
    "        f.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Translation complete! Saved as 'massive_amr_irish.jsonl'\n"
     ]
    }
   ],
   "source": [
    "print(\"✅ Translation complete! Saved as 'massive_amr_irish.jsonl'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's try to train the AMR Parser on Welsh - T5-base did not worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8.0\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import amrlib\n",
    "print(amrlib.__version__)  # Should print the version number\n",
    "print(hasattr(amrlib, 'parse_string')) # Should print True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import gc\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForSeq2Seq\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Free GPU memory before training\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "file_path = \"data/massive_amr_welsh.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_amr_data(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        data = [json.loads(line) for line in file]\n",
    "    return [entry for entry in data if entry.get(\"raw_amr\") and entry.get(\"utt\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_amr_data(file_path)\n",
    "random.shuffle(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Data\n",
    "sentences = [entry[\"utt\"] for entry in data]\n",
    "amrs = [entry[\"raw_amr\"] for entry in data]\n",
    "train_sents, test_sents, train_amrs, test_amrs = train_test_split(sentences, amrs, test_size=0.1, random_state=42)\n",
    "train_sents, val_sents, train_amrs, val_amrs = train_test_split(train_sents, train_amrs, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Hugging Face Dataset format\n",
    "def create_dataset(sentences, amrs):\n",
    "    return Dataset.from_dict({\"sentence\": sentences, \"amr\": amrs})\n",
    "\n",
    "datasets = DatasetDict({\n",
    "    \"train\": create_dataset(train_sents, train_amrs),\n",
    "    \"validation\": create_dataset(val_sents, val_amrs),\n",
    "    \"test\": create_dataset(test_sents, test_amrs),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "# Load T5 Model\n",
    "model_name = \"t5-base\"  # Use \"t5-small\" if GPU still struggles\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable Gradient Checkpointing (Reduces Memory Usage)\n",
    "model.gradient_checkpointing_enable()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization Function\n",
    "def preprocess_function(examples):\n",
    "    inputs = [\"parse: \" + ex for ex in examples[\"sentence\"]]\n",
    "    targets = examples[\"amr\"]\n",
    "    \n",
    "    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n",
    "    labels = tokenizer(targets, max_length=1024, truncation=True, padding=\"max_length\")\n",
    "    \n",
    "    # Replace padding token id's in labels with -100 so they are ignored by the loss function\n",
    "    labels[\"input_ids\"] = [\n",
    "        [l if l != tokenizer.pad_token_id else -100 for l in label]\n",
    "        for label in labels[\"input_ids\"]\n",
    "    ]\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52dff023f62142169d398a87933a25db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1364 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74f3945425884d85b657a314900cea9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/152 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41e8fb86c37448ebb8d2f3ef0216b97b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/169 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply tokenization\n",
    "tokenized_datasets = datasets.map(preprocess_function, batched=True, remove_columns=[\"sentence\", \"amr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\stoic\\miniconda3\\envs\\llms_env\\lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Optimized Training Arguments for RTX 3070 Ti\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./amr_t5_model_optimized\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-4,  # Lower LR for stability\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    warmup_steps=500,\n",
    "    per_device_train_batch_size=2,  # Reduce batch size\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=8,  # Adjusted to reduce memory usage\n",
    "    num_train_epochs=10,  # Reduced epochs to avoid overfitting\n",
    "    weight_decay=0.03,  # Regularization\n",
    "    save_total_limit=2,\n",
    "    logging_steps=50,\n",
    "    logging_dir=\"./logs\",\n",
    "    report_to=\"none\",\n",
    "    fp16=True,  # Mixed Precision Training (Lowers Memory Usage)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stoic\\AppData\\Local\\Temp\\ipykernel_18480\\4006242607.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# Trainer Setup\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "187"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting Optimized T5 Training for AMR Parsing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8' max='850' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  8/850 00:28 < 1:06:31, 0.21 it/s, Epoch 0.08/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🚀 Starting Optimized T5 Training for AMR Parsing...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\stoic\\miniconda3\\envs\\llms_env\\lib\\site-packages\\transformers\\trainer.py:2171\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2169\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2170\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2172\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2176\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\stoic\\miniconda3\\envs\\llms_env\\lib\\site-packages\\transformers\\trainer.py:2531\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2524\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2525\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m   2526\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2527\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[0;32m   2528\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[0;32m   2529\u001b[0m )\n\u001b[0;32m   2530\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m-> 2531\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2533\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2534\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2535\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2536\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2537\u001b[0m ):\n\u001b[0;32m   2538\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2539\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\stoic\\miniconda3\\envs\\llms_env\\lib\\site-packages\\transformers\\trainer.py:3712\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   3709\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_accepts_loss_kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3710\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n\u001b[1;32m-> 3712\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mbackward(loss, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   3714\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[1;32mc:\\Users\\stoic\\miniconda3\\envs\\llms_env\\lib\\site-packages\\accelerate\\accelerator.py:2242\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[1;34m(self, loss, **kwargs)\u001b[0m\n\u001b[0;32m   2240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   2241\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 2242\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   2243\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m learning_rate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_lomo_optimizer:\n\u001b[0;32m   2244\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n",
      "File \u001b[1;32mc:\\Users\\stoic\\miniconda3\\envs\\llms_env\\lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\stoic\\miniconda3\\envs\\llms_env\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\stoic\\miniconda3\\envs\\llms_env\\lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"🚀 Starting Optimized T5 Training for AMR Parsing...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Columns: ['sentence', 'amr']\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset Columns:\", datasets[\"test\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Columns (after tokenization): ['input_ids', 'attention_mask', 'labels', 'decoder_input_ids']\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset Columns (after tokenization):\", tokenized_datasets[\"test\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./fine_tuned_amr_t5_2\\\\tokenizer_config.json',\n",
       " './fine_tuned_amr_t5_2\\\\special_tokens_map.json',\n",
       " './fine_tuned_amr_t5_2\\\\spiece.model',\n",
       " './fine_tuned_amr_t5_2\\\\added_tokens.json')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# 9. Save Fine-Tuned Model\n",
    "# ----------------------------\n",
    "model.save_pretrained(\"./fine_tuned_amr_t5_2\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_amr_t5_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Smatch Score - T5-base did not worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import smatch\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# ⿡ Load Fine-Tuned T5 Model & Tokenizer\n",
    "# -----------------------------------\n",
    "model_path = \"./fine_tuned_amr_t5_2\"\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 768)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Move model to GPU if available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# ⿢ Load Test Data\n",
    "# -----------------------------------\n",
    "file_path = \"data/massive_amr_welsh.jsonl\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_amr_data(file_path):\n",
    "    \"\"\"Loads JSONL AMR dataset and removes missing entries.\"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        data = [json.loads(line) for line in file]\n",
    "    return [entry for entry in data if entry.get(\"raw_amr\") and entry.get(\"utt\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "data = load_amr_data(file_path)\n",
    "sentences = [entry[\"utt\"] for entry in data]\n",
    "gold_amrs = [entry[\"raw_amr\"] for entry in data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Hugging Face Dataset format\n",
    "test_dataset = Dataset.from_dict({\"sentence\": sentences, \"amr\": gold_amrs})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# ⿣ Improved AMR Cleaning Function (Fix Duplicate Names)\n",
    "# -----------------------------------\n",
    "def fix_duplicate_nodes(amr_text):\n",
    "    \"\"\"Renames duplicate variables dynamically.\"\"\"\n",
    "    if not amr_text.strip():\n",
    "        return \"INVALID_AMR\"\n",
    "\n",
    "    used_vars = defaultdict(int)  # Tracks occurrences of each variable\n",
    "    renamed_vars = {}\n",
    "\n",
    "    def rename_variable(match):\n",
    "        var_name = match.group(1)\n",
    "        if var_name in used_vars:\n",
    "            new_var_name = f\"{var_name}_{used_vars[var_name]}\"\n",
    "            used_vars[var_name] += 1\n",
    "            renamed_vars[var_name] = new_var_name\n",
    "            return f\"({new_var_name} /\"\n",
    "        else:\n",
    "            used_vars[var_name] = 1\n",
    "            return f\"({var_name} /\"\n",
    "\n",
    "    # Ensure variables like (t2 / thing) are renamed uniquely\n",
    "    amr_text = re.sub(r\"\\((\\w+)\\s+\\/\", rename_variable, amr_text)\n",
    "\n",
    "    # Replace occurrences of renamed variables in AMR text\n",
    "    for old_var, new_var in renamed_vars.items():\n",
    "        amr_text = amr_text.replace(f\" {old_var} \", f\" {new_var} \")\n",
    "\n",
    "    return amr_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# ⿤ Generate AMR Predictions (Fixed)\n",
    "# -----------------------------------\n",
    "def generate_amr_predictions(model, tokenizer, test_dataset, max_samples=100):\n",
    "    \"\"\"Generates AMR graphs from input sentences using the fine-tuned model.\"\"\"\n",
    "    predictions = []\n",
    "\n",
    "    for i, example in enumerate(test_dataset):\n",
    "        if i >= max_samples:  # Limit number of samples for efficiency\n",
    "            break\n",
    "\n",
    "        input_text = \"parse: \" + example[\"sentence\"]\n",
    "        input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(input_ids, max_length=512)\n",
    "        \n",
    "        predicted_amr = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        predicted_amr = fix_duplicate_nodes(predicted_amr)  # ✅ Ensure AMR is valid\n",
    "\n",
    "        gold_amr = fix_duplicate_nodes(example[\"amr\"])  # ✅ Fix duplicates in gold AMRs too\n",
    "\n",
    "        predictions.append((gold_amr, predicted_amr))\n",
    "\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "predictions = generate_amr_predictions(model, tokenizer, test_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# ⿥ Compute Smatch Score (with Fixes)\n",
    "# -----------------------------------\n",
    "def is_valid_amr(amr):\n",
    "    \"\"\"Checks if AMR is valid by ensuring it has at least one '/' symbol.\"\"\"\n",
    "    return amr.count('/') > 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_smatch(gold_amrs, predicted_amrs):\n",
    "    \"\"\"Computes the Smatch score while handling errors.\"\"\"\n",
    "    total_precision, total_recall, total_f1 = 0, 0, 0\n",
    "    valid_samples = 0\n",
    "\n",
    "    for i, (gold, pred) in enumerate(zip(gold_amrs, predicted_amrs)):\n",
    "        try:\n",
    "            # ✅ Ensure AMRs are valid\n",
    "            if not is_valid_amr(pred) or not is_valid_amr(gold):\n",
    "                print(f\"⚠ Skipping sample {i} due to invalid AMR\")\n",
    "                continue\n",
    "\n",
    "            precision, recall, f_score = smatch.get_amr_match(str(gold), str(pred))\n",
    "            total_precision += precision\n",
    "            total_recall += recall\n",
    "            total_f1 += f_score\n",
    "            valid_samples += 1\n",
    "        except Exception as e:\n",
    "            print(f\"⚠ Error in Smatch calculation for sample {i}: {e}\")\n",
    "            print(f\"Gold AMR: {gold}\")\n",
    "            print(f\"Predicted AMR: {pred}\")\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "    if valid_samples == 0:\n",
    "        return 0, 0, 0  # Avoid division by zero\n",
    "\n",
    "    return total_precision / valid_samples, total_recall / valid_samples, total_f1 / valid_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract predicted AMRs for Smatch evaluation\n",
    "gold_amrs = [gold for gold, _ in predictions if gold is not None]\n",
    "predicted_amrs = [pred for _, pred in predictions if pred is not None]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ Skipping sample 0 due to invalid AMR\n",
      "⚠ Skipping sample 1 due to invalid AMR\n",
      "⚠ Skipping sample 2 due to invalid AMR\n",
      "⚠ Skipping sample 3 due to invalid AMR\n",
      "⚠ Skipping sample 4 due to invalid AMR\n",
      "⚠ Skipping sample 5 due to invalid AMR\n",
      "⚠ Skipping sample 6 due to invalid AMR\n",
      "⚠ Skipping sample 7 due to invalid AMR\n",
      "⚠ Skipping sample 8 due to invalid AMR\n",
      "⚠ Skipping sample 9 due to invalid AMR\n",
      "⚠ Skipping sample 10 due to invalid AMR\n",
      "⚠ Skipping sample 11 due to invalid AMR\n",
      "⚠ Skipping sample 12 due to invalid AMR\n",
      "⚠ Skipping sample 13 due to invalid AMR\n",
      "⚠ Skipping sample 14 due to invalid AMR\n",
      "⚠ Skipping sample 15 due to invalid AMR\n",
      "⚠ Skipping sample 16 due to invalid AMR\n",
      "⚠ Skipping sample 17 due to invalid AMR\n",
      "⚠ Skipping sample 18 due to invalid AMR\n",
      "⚠ Skipping sample 19 due to invalid AMR\n",
      "⚠ Skipping sample 20 due to invalid AMR\n",
      "⚠ Skipping sample 21 due to invalid AMR\n",
      "⚠ Skipping sample 22 due to invalid AMR\n",
      "⚠ Skipping sample 23 due to invalid AMR\n",
      "⚠ Skipping sample 24 due to invalid AMR\n",
      "⚠ Skipping sample 25 due to invalid AMR\n",
      "⚠ Skipping sample 26 due to invalid AMR\n",
      "⚠ Skipping sample 27 due to invalid AMR\n",
      "⚠ Skipping sample 28 due to invalid AMR\n",
      "⚠ Skipping sample 29 due to invalid AMR\n",
      "⚠ Skipping sample 30 due to invalid AMR\n",
      "⚠ Skipping sample 31 due to invalid AMR\n",
      "⚠ Skipping sample 32 due to invalid AMR\n",
      "⚠ Skipping sample 33 due to invalid AMR\n",
      "⚠ Skipping sample 34 due to invalid AMR\n",
      "⚠ Skipping sample 35 due to invalid AMR\n",
      "⚠ Skipping sample 36 due to invalid AMR\n",
      "⚠ Skipping sample 37 due to invalid AMR\n",
      "⚠ Skipping sample 38 due to invalid AMR\n",
      "⚠ Skipping sample 39 due to invalid AMR\n",
      "⚠ Skipping sample 40 due to invalid AMR\n",
      "⚠ Skipping sample 41 due to invalid AMR\n",
      "⚠ Skipping sample 42 due to invalid AMR\n",
      "⚠ Skipping sample 43 due to invalid AMR\n",
      "⚠ Skipping sample 44 due to invalid AMR\n",
      "⚠ Skipping sample 45 due to invalid AMR\n",
      "⚠ Skipping sample 46 due to invalid AMR\n",
      "⚠ Skipping sample 47 due to invalid AMR\n",
      "⚠ Skipping sample 48 due to invalid AMR\n",
      "⚠ Skipping sample 49 due to invalid AMR\n",
      "⚠ Skipping sample 50 due to invalid AMR\n",
      "⚠ Skipping sample 51 due to invalid AMR\n",
      "⚠ Skipping sample 52 due to invalid AMR\n",
      "⚠ Skipping sample 53 due to invalid AMR\n",
      "⚠ Skipping sample 54 due to invalid AMR\n",
      "⚠ Skipping sample 55 due to invalid AMR\n",
      "⚠ Skipping sample 56 due to invalid AMR\n",
      "⚠ Skipping sample 57 due to invalid AMR\n",
      "⚠ Skipping sample 58 due to invalid AMR\n",
      "⚠ Skipping sample 59 due to invalid AMR\n",
      "⚠ Skipping sample 60 due to invalid AMR\n",
      "⚠ Skipping sample 61 due to invalid AMR\n",
      "⚠ Skipping sample 62 due to invalid AMR\n",
      "⚠ Skipping sample 63 due to invalid AMR\n",
      "⚠ Skipping sample 64 due to invalid AMR\n",
      "⚠ Skipping sample 65 due to invalid AMR\n",
      "⚠ Skipping sample 66 due to invalid AMR\n",
      "⚠ Skipping sample 67 due to invalid AMR\n",
      "⚠ Skipping sample 68 due to invalid AMR\n",
      "⚠ Skipping sample 69 due to invalid AMR\n",
      "⚠ Skipping sample 70 due to invalid AMR\n",
      "⚠ Skipping sample 71 due to invalid AMR\n",
      "⚠ Skipping sample 72 due to invalid AMR\n",
      "⚠ Skipping sample 73 due to invalid AMR\n",
      "⚠ Skipping sample 74 due to invalid AMR\n",
      "⚠ Skipping sample 75 due to invalid AMR\n",
      "⚠ Skipping sample 76 due to invalid AMR\n",
      "⚠ Skipping sample 77 due to invalid AMR\n",
      "⚠ Skipping sample 78 due to invalid AMR\n",
      "⚠ Skipping sample 79 due to invalid AMR\n",
      "⚠ Skipping sample 80 due to invalid AMR\n",
      "⚠ Skipping sample 81 due to invalid AMR\n",
      "⚠ Skipping sample 82 due to invalid AMR\n",
      "⚠ Skipping sample 83 due to invalid AMR\n",
      "⚠ Skipping sample 84 due to invalid AMR\n",
      "⚠ Skipping sample 85 due to invalid AMR\n",
      "⚠ Skipping sample 86 due to invalid AMR\n",
      "⚠ Skipping sample 87 due to invalid AMR\n",
      "⚠ Skipping sample 88 due to invalid AMR\n",
      "⚠ Skipping sample 89 due to invalid AMR\n",
      "⚠ Skipping sample 90 due to invalid AMR\n",
      "⚠ Skipping sample 91 due to invalid AMR\n",
      "⚠ Skipping sample 92 due to invalid AMR\n",
      "⚠ Skipping sample 93 due to invalid AMR\n",
      "⚠ Skipping sample 94 due to invalid AMR\n",
      "⚠ Skipping sample 95 due to invalid AMR\n",
      "⚠ Skipping sample 96 due to invalid AMR\n",
      "⚠ Skipping sample 97 due to invalid AMR\n",
      "⚠ Skipping sample 98 due to invalid AMR\n",
      "⚠ Skipping sample 99 due to invalid AMR\n"
     ]
    }
   ],
   "source": [
    "# Compute Smatch Score\n",
    "precision, recall, f1 = compute_smatch(gold_amrs, predicted_amrs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 Smatch Score - Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------\n",
    "# ⿦ Display Results\n",
    "# -----------------------------------\n",
    "print(f\"🔥 Smatch Score - Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Dataset for Corruption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "file_path = \"data/massive_amr_welsh.jsonl\"\n",
    "valid_entries = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            entry = json.loads(line)\n",
    "            if \"utt\" in entry and \"raw_amr\" in entry and isinstance(entry[\"utt\"], str) and isinstance(entry[\"raw_amr\"], str):\n",
    "                valid_entries.append(entry)\n",
    "            else:\n",
    "                print(f\"❌ Corrupt entry found and skipped: {entry}\")\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"❌ JSON Decode Error in line: {line}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Valid samples: 1685\n"
     ]
    }
   ],
   "source": [
    "print(f\"✅ Valid samples: {len(valid_entries)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Cleaned dataset saved as 'data/cleaned_amr_welsh.jsonl'\n"
     ]
    }
   ],
   "source": [
    "# Save cleaned dataset\n",
    "with open(\"data/cleaned_amr_welsh.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for entry in valid_entries:\n",
    "        f.write(json.dumps(entry) + \"\\n\")\n",
    "\n",
    "print(\"✅ Cleaned dataset saved as 'data/cleaned_amr_welsh.jsonl'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Good Smatch Score for Welsh - T5-small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ Error in Smatch calculation for sample 7: 'NoneType' object has no attribute 'rename_node'\n",
      "Gold AMR: (r / rate-01\n",
      "      :ARG1 (e / exchange-01\n",
      "            :ARG1 (c / currency :name (n / name :op1 \"u.\" :op2 \"s.\" :op3 \"d.\"))\n",
      "            :ARG3 (c2 / currency :name (n2 / name :op1 \"cdn\")))\n",
      "      :ARG2 (a / amr-unknown))\n",
      "Predicted AMR: (r / rate-01 :ARG1 (e / exchange-01 :ARG1 (c / currency :name (n / name :op1 \"u.\" :op2 \"s.\" :op3 \"d.\")) :ARG3 (c2 / currency :name (n2 / name :op1 \"d.\"))) :ARG3 (c2_1 / currency :name (n2_1 / name :op1 \"euro\"))))\n",
      "--------------------------------------------------\n",
      "⚠ Error in Smatch calculation for sample 12: not enough values to unpack (expected 2, got 1)\n",
      "Gold AMR: (t / tell-01 :mode imperative\n",
      "      :ARG0 (y / you)\n",
      "      :ARG1 (t2 / thing\n",
      "            :mod (a2 / all)\n",
      "            :topic (h / hurricane-01))\n",
      "      :ARG2 (i / i))\n",
      "Predicted AMR: (t / tell-01 :mode imperative :ARG0 (y / you) :ARG1 (t2 / thing :ARG2-of (p / person :name (n / name :op1 \"abraham\" :op2 \"s.\" :op3 \"s.\" :op3 \"s.\" :op3 \"m.\")))) :ARG2 (i / i))\n",
      "--------------------------------------------------\n",
      "⚠ Error in Smatch calculation for sample 14: not enough values to unpack (expected 2, got 1)\n",
      "Gold AMR: (r / rate-01\n",
      "      :ARG1 (e / exchange-01\n",
      "            :ARG1 (c / currency :name (n / name :op1 \"us\" :op2 \"dollar\"))\n",
      "            :ARG3 (c2 / currency :name (n2 / name :op1 \"euro\")))\n",
      "      :ARG2 (a / amr-unknown))\n",
      "Predicted AMR: (r / rate-01 :ARG1 (e / exchange-01 :ARG1 (c / currency :name (n / name :op1 \"euro\")) :ARG3 (c2 / currency :name (n2 / name :op1 \"euro\"))) :ARG3 (c2_1 / currency :name (n2_1 / name :op1 \"euro\"))) :ARG2 (a / amr-unknown))\n",
      "--------------------------------------------------\n",
      "⚠ Error in Smatch calculation for sample 28: not enough values to unpack (expected 2, got 1)\n",
      "Gold AMR: (t / tell-01 :mode imperative :polite +\n",
      "      :ARG0 (y / you)\n",
      "      :ARG1 (t2 / thing\n",
      "            :domain (c / chaise))\n",
      "      :ARG2 (i / i))\n",
      "Predicted AMR: (t / tell-01 :mode imperative :polite + :ARG0 (y / you) :ARG1 (t2 / thing :ARG2-of (p / person :name (n / name :op1 \"elvis\")))) :ARG2 (i / i))\n",
      "--------------------------------------------------\n",
      "⚠ Error in Smatch calculation for sample 95: not enough values to unpack (expected 2, got 1)\n",
      "Gold AMR: (t / tell-01 :mode imperative\n",
      "      :ARG0 (y / you)\n",
      "      :ARG1 (t2 / thing\n",
      "            :topic (f / fungi :name (n / name :op1 \"morel\" :op2 \"mushrooms\")))\n",
      "      :ARG2 (i / i))\n",
      "Predicted AMR: (t / tell-01 :mode imperative :ARG0 (y / you) :ARG1 (t2 / thing :ARG2-of (p / person :name (n / name :op1 \"abraham\" :op2 \"seven\")))) :ARG2 (i / i))\n",
      "--------------------------------------------------\n",
      "🔥 Smatch Score - Precision: 5.0632, Recall: 11.8316, F1: 13.4421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unmatched parenthesis at position 211 in processing (r / rate-01 :ARG1 (e / exchange-01 :ARG1 (c / currency :name (n / name :op1 \"u.\" :op2 \"s.\" :op3 \"d.\")) :ARG3 (c2 / currency :name (n2 / name :op1 \"d.\"))) :ARG3 (c2_1 / currency :name (n2_1 / name :op1 \"euro\"))))\n",
      "Error in parsing amr 2: (t / tell-01 :mode imperative :ARG0 (y / you) :ARG1 (t2 / thing :ARG2-of (p / person :name (n / name :op1 \"abraham\" :op2 \"s.\" :op3 \"s.\" :op3 \"s.\" :op3 \"m.\")))) :ARG2 (i / i))\n",
      "Please check if the AMR is ill-formatted. Ignoring remaining AMRs\n",
      "Error message: list index out of range\n",
      "Error in parsing amr 2: (r / rate-01 :ARG1 (e / exchange-01 :ARG1 (c / currency :name (n / name :op1 \"euro\")) :ARG3 (c2 / currency :name (n2 / name :op1 \"euro\"))) :ARG3 (c2_1 / currency :name (n2_1 / name :op1 \"euro\"))) :ARG2 (a / amr-unknown))\n",
      "Please check if the AMR is ill-formatted. Ignoring remaining AMRs\n",
      "Error message: list index out of range\n",
      "Error in parsing amr 2: (t / tell-01 :mode imperative :polite + :ARG0 (y / you) :ARG1 (t2 / thing :ARG2-of (p / person :name (n / name :op1 \"elvis\")))) :ARG2 (i / i))\n",
      "Please check if the AMR is ill-formatted. Ignoring remaining AMRs\n",
      "Error message: list index out of range\n",
      "Error in parsing amr 2: (t / tell-01 :mode imperative :ARG0 (y / you) :ARG1 (t2 / thing :ARG2-of (p / person :name (n / name :op1 \"abraham\" :op2 \"seven\")))) :ARG2 (i / i))\n",
      "Please check if the AMR is ill-formatted. Ignoring remaining AMRs\n",
      "Error message: list index out of range\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import smatch\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "from datasets import Dataset\n",
    "\n",
    "# -----------------------------------\n",
    "# ⿡ Load Fine-Tuned T5 Model & Tokenizer\n",
    "# -----------------------------------\n",
    "model_path = \"./fine_tuned_amr_t5\"\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# -----------------------------------\n",
    "# ⿢ Load Test Data\n",
    "# -----------------------------------\n",
    "file_path = \"data/massive_amr_welsh.jsonl\"\n",
    "\n",
    "def load_amr_data(file_path):\n",
    "    \"\"\"Loads JSONL AMR dataset and removes missing entries.\"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        data = [json.loads(line) for line in file]\n",
    "    return [entry for entry in data if entry.get(\"raw_amr\") and entry.get(\"utt\")]\n",
    "\n",
    "# Load test data\n",
    "data = load_amr_data(file_path)\n",
    "sentences = [entry[\"utt\"] for entry in data]\n",
    "gold_amrs = [entry[\"raw_amr\"] for entry in data]\n",
    "\n",
    "# Convert to Hugging Face Dataset format\n",
    "test_dataset = Dataset.from_dict({\"sentence\": sentences, \"amr\": gold_amrs})\n",
    "\n",
    "# -----------------------------------\n",
    "# ⿣ Improved AMR Cleaning Function (Fix Duplicate Names)\n",
    "# -----------------------------------\n",
    "def fix_duplicate_nodes(amr_text):\n",
    "    \"\"\"Renames duplicate variables dynamically.\"\"\"\n",
    "    if not amr_text.strip():\n",
    "        return \"INVALID_AMR\"\n",
    "\n",
    "    used_vars = defaultdict(int)  # Tracks occurrences of each variable\n",
    "    renamed_vars = {}\n",
    "\n",
    "    def rename_variable(match):\n",
    "        var_name = match.group(1)\n",
    "        if var_name in used_vars:\n",
    "            new_var_name = f\"{var_name}_{used_vars[var_name]}\"\n",
    "            used_vars[var_name] += 1\n",
    "            renamed_vars[var_name] = new_var_name\n",
    "            return f\"({new_var_name} /\"\n",
    "        else:\n",
    "            used_vars[var_name] = 1\n",
    "            return f\"({var_name} /\"\n",
    "\n",
    "    # Ensure variables like (t2 / thing) are renamed uniquely\n",
    "    amr_text = re.sub(r\"\\((\\w+)\\s+\\/\", rename_variable, amr_text)\n",
    "\n",
    "    # Replace occurrences of renamed variables in AMR text\n",
    "    for old_var, new_var in renamed_vars.items():\n",
    "        amr_text = amr_text.replace(f\" {old_var} \", f\" {new_var} \")\n",
    "\n",
    "    return amr_text\n",
    "\n",
    "# -----------------------------------\n",
    "# ⿤ Generate AMR Predictions (Fixed)\n",
    "# -----------------------------------\n",
    "def generate_amr_predictions(model, tokenizer, test_dataset, max_samples=100):\n",
    "    \"\"\"Generates AMR graphs from input sentences using the fine-tuned model.\"\"\"\n",
    "    predictions = []\n",
    "\n",
    "    for i, example in enumerate(test_dataset):\n",
    "        if i >= max_samples:  # Limit number of samples for efficiency\n",
    "            break\n",
    "\n",
    "        input_text = \"parse: \" + example[\"sentence\"]\n",
    "        input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(input_ids, max_length=512)\n",
    "        \n",
    "        predicted_amr = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        predicted_amr = fix_duplicate_nodes(predicted_amr)  # ✅ Ensure AMR is valid\n",
    "\n",
    "        gold_amr = fix_duplicate_nodes(example[\"amr\"])  # ✅ Fix duplicates in gold AMRs too\n",
    "\n",
    "        predictions.append((gold_amr, predicted_amr))\n",
    "\n",
    "    return predictions\n",
    "\n",
    "# Generate predictions\n",
    "predictions = generate_amr_predictions(model, tokenizer, test_dataset)\n",
    "\n",
    "# -----------------------------------\n",
    "# ⿥ Compute Smatch Score (with Fixes)\n",
    "# -----------------------------------\n",
    "def is_valid_amr(amr):\n",
    "    \"\"\"Checks if AMR is valid by ensuring it has at least one '/' symbol.\"\"\"\n",
    "    return amr.count('/') > 0\n",
    "\n",
    "def compute_smatch(gold_amrs, predicted_amrs):\n",
    "    \"\"\"Computes the Smatch score while handling errors.\"\"\"\n",
    "    total_precision, total_recall, total_f1 = 0, 0, 0\n",
    "    valid_samples = 0\n",
    "\n",
    "    for i, (gold, pred) in enumerate(zip(gold_amrs, predicted_amrs)):\n",
    "        try:\n",
    "            # ✅ Ensure AMRs are valid\n",
    "            if not is_valid_amr(pred) or not is_valid_amr(gold):\n",
    "                print(f\"⚠ Skipping sample {i} due to invalid AMR\")\n",
    "                continue\n",
    "\n",
    "            precision, recall, f_score = smatch.get_amr_match(str(gold), str(pred))\n",
    "            total_precision += precision\n",
    "            total_recall += recall\n",
    "            total_f1 += f_score\n",
    "            valid_samples += 1\n",
    "        except Exception as e:\n",
    "            print(f\"⚠ Error in Smatch calculation for sample {i}: {e}\")\n",
    "            print(f\"Gold AMR: {gold}\")\n",
    "            print(f\"Predicted AMR: {pred}\")\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "    if valid_samples == 0:\n",
    "        return 0, 0, 0  # Avoid division by zero\n",
    "\n",
    "    return total_precision / valid_samples, total_recall / valid_samples, total_f1 / valid_samples\n",
    "\n",
    "# Extract predicted AMRs for Smatch evaluation\n",
    "gold_amrs = [gold for gold, _ in predictions if gold is not None]\n",
    "predicted_amrs = [pred for _, pred in predictions if pred is not None]\n",
    "\n",
    "# Compute Smatch Score\n",
    "precision, recall, f1 = compute_smatch(gold_amrs, predicted_amrs)\n",
    "\n",
    "# -----------------------------------\n",
    "# ⿦ Display Results\n",
    "# -----------------------------------\n",
    "print(f\"🔥 Smatch Score - Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smatch score for T5-base - did not worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 98\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m predictions\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# Generate predictions\u001b[39;00m\n\u001b[1;32m---> 98\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_amr_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;66;03m# -----------------------------------\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;66;03m# ⿥ Compute Smatch Score (with Fixes)\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;66;03m# -----------------------------------\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_valid_amr\u001b[39m(amr):\n",
      "Cell \u001b[1;32mIn[36], line 86\u001b[0m, in \u001b[0;36mgenerate_amr_predictions\u001b[1;34m(model, tokenizer, test_dataset, max_samples)\u001b[0m\n\u001b[0;32m     83\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(input_text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 86\u001b[0m     output_ids \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     88\u001b[0m predicted_amr \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(output_ids[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     89\u001b[0m predicted_amr \u001b[38;5;241m=\u001b[39m fix_duplicate_nodes(predicted_amr)  \u001b[38;5;66;03m# ✅ Ensure AMR is valid\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\stoic\\miniconda3\\envs\\llms_env\\lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\stoic\\miniconda3\\envs\\llms_env\\lib\\site-packages\\transformers\\generation\\utils.py:2255\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   2247\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   2248\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   2249\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[0;32m   2250\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   2251\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2252\u001b[0m     )\n\u001b[0;32m   2254\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[1;32m-> 2255\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sample(\n\u001b[0;32m   2256\u001b[0m         input_ids,\n\u001b[0;32m   2257\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[0;32m   2258\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mprepared_stopping_criteria,\n\u001b[0;32m   2259\u001b[0m         generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[0;32m   2260\u001b[0m         synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[0;32m   2261\u001b[0m         streamer\u001b[38;5;241m=\u001b[39mstreamer,\n\u001b[0;32m   2262\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2263\u001b[0m     )\n\u001b[0;32m   2265\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[0;32m   2266\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[0;32m   2267\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[0;32m   2268\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   2269\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2274\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[0;32m   2275\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\stoic\\miniconda3\\envs\\llms_env\\lib\\site-packages\\transformers\\generation\\utils.py:3257\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[0;32m   3255\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   3256\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   3259\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[0;32m   3260\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[0;32m   3261\u001b[0m     outputs,\n\u001b[0;32m   3262\u001b[0m     model_kwargs,\n\u001b[0;32m   3263\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   3264\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\stoic\\miniconda3\\envs\\llms_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\stoic\\miniconda3\\envs\\llms_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\stoic\\miniconda3\\envs\\llms_env\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:1891\u001b[0m, in \u001b[0;36mT5ForConditionalGeneration.forward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[0;32m   1888\u001b[0m         decoder_attention_mask \u001b[38;5;241m=\u001b[39m decoder_attention_mask\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39mfirst_device)\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;66;03m# Decode\u001b[39;00m\n\u001b[1;32m-> 1891\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1892\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1893\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1894\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1895\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1896\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1897\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1898\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1899\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1900\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1901\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1902\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1903\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1904\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1905\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1907\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m decoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1909\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\stoic\\miniconda3\\envs\\llms_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\stoic\\miniconda3\\envs\\llms_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\stoic\\miniconda3\\envs\\llms_env\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:1124\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[1;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[0;32m   1107\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m   1108\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39mforward,\n\u001b[0;32m   1109\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1121\u001b[0m         cache_position,\n\u001b[0;32m   1122\u001b[0m     )\n\u001b[0;32m   1123\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1124\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1128\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1129\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1130\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1131\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1132\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1133\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1134\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1135\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1136\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1137\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1138\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1140\u001b[0m \u001b[38;5;66;03m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[0;32m   1141\u001b[0m \u001b[38;5;66;03m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n\u001b[0;32m   1142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\stoic\\miniconda3\\envs\\llms_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\stoic\\miniconda3\\envs\\llms_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\stoic\\miniconda3\\envs\\llms_env\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:699\u001b[0m, in \u001b[0;36mT5Block.forward\u001b[1;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict, cache_position)\u001b[0m\n\u001b[0;32m    697\u001b[0m do_cross_attention \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_decoder \u001b[38;5;129;01mand\u001b[39;00m encoder_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_cross_attention:\n\u001b[1;32m--> 699\u001b[0m     cross_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_value_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    703\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    704\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    706\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    707\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    708\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    709\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    710\u001b[0m     hidden_states, past_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[:\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m    712\u001b[0m     \u001b[38;5;66;03m# clamp inf values to enable fp16 training\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\stoic\\miniconda3\\envs\\llms_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\stoic\\miniconda3\\envs\\llms_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\stoic\\miniconda3\\envs\\llms_env\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:629\u001b[0m, in \u001b[0;36mT5LayerCrossAttention.forward\u001b[1;34m(self, hidden_states, key_value_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, query_length, output_attentions, cache_position)\u001b[0m\n\u001b[0;32m    615\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    616\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    617\u001b[0m     hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    626\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    627\u001b[0m ):\n\u001b[0;32m    628\u001b[0m     normed_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(hidden_states)\n\u001b[1;32m--> 629\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEncDecAttention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    630\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnormed_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_value_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_value_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    633\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    635\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    636\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    637\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    638\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    639\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    640\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    641\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attention_output[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m    642\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m attention_output[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\stoic\\miniconda3\\envs\\llms_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\stoic\\miniconda3\\envs\\llms_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\stoic\\miniconda3\\envs\\llms_env\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:559\u001b[0m, in \u001b[0;36mT5Attention.forward\u001b[1;34m(self, hidden_states, mask, key_value_states, position_bias, past_key_value, layer_head_mask, query_length, use_cache, output_attentions, cache_position)\u001b[0m\n\u001b[0;32m    556\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m layer_head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    557\u001b[0m     attn_weights \u001b[38;5;241m=\u001b[39m attn_weights \u001b[38;5;241m*\u001b[39m layer_head_mask\n\u001b[1;32m--> 559\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    561\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m    562\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mview(batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minner_dim)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import smatch\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "from datasets import Dataset\n",
    "\n",
    "# -----------------------------------\n",
    "# ⿡ Load Fine-Tuned T5 Model & Tokenizer\n",
    "# -----------------------------------\n",
    "model_path = \"./fine_tuned_amr_t5_2\"\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# -----------------------------------\n",
    "# ⿢ Load Test Data\n",
    "# -----------------------------------\n",
    "file_path = \"data/massive_amr_welsh.jsonl\"\n",
    "\n",
    "def load_amr_data(file_path):\n",
    "    \"\"\"Loads JSONL AMR dataset and removes missing entries.\"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        data = [json.loads(line) for line in file]\n",
    "    return [entry for entry in data if entry.get(\"raw_amr\") and entry.get(\"utt\")]\n",
    "\n",
    "# Load test data\n",
    "data = load_amr_data(file_path)\n",
    "sentences = [entry[\"utt\"] for entry in data]\n",
    "gold_amrs = [entry[\"raw_amr\"] for entry in data]\n",
    "\n",
    "# Convert to Hugging Face Dataset format\n",
    "test_dataset = Dataset.from_dict({\"sentence\": sentences, \"amr\": gold_amrs})\n",
    "\n",
    "# -----------------------------------\n",
    "# ⿣ Improved AMR Cleaning Function (Fix Duplicate Names)\n",
    "# -----------------------------------\n",
    "def fix_duplicate_nodes(amr_text):\n",
    "    \"\"\"Renames duplicate variables dynamically.\"\"\"\n",
    "    if not amr_text.strip():\n",
    "        return \"INVALID_AMR\"\n",
    "\n",
    "    used_vars = defaultdict(int)  # Tracks occurrences of each variable\n",
    "    renamed_vars = {}\n",
    "\n",
    "    def rename_variable(match):\n",
    "        var_name = match.group(1)\n",
    "        if var_name in used_vars:\n",
    "            new_var_name = f\"{var_name}_{used_vars[var_name]}\"\n",
    "            used_vars[var_name] += 1\n",
    "            renamed_vars[var_name] = new_var_name\n",
    "            return f\"({new_var_name} /\"\n",
    "        else:\n",
    "            used_vars[var_name] = 1\n",
    "            return f\"({var_name} /\"\n",
    "\n",
    "    # Ensure variables like (t2 / thing) are renamed uniquely\n",
    "    amr_text = re.sub(r\"\\((\\w+)\\s+\\/\", rename_variable, amr_text)\n",
    "\n",
    "    # Replace occurrences of renamed variables in AMR text\n",
    "    for old_var, new_var in renamed_vars.items():\n",
    "        amr_text = amr_text.replace(f\" {old_var} \", f\" {new_var} \")\n",
    "\n",
    "    return amr_text\n",
    "\n",
    "# -----------------------------------\n",
    "# ⿤ Generate AMR Predictions (Fixed)\n",
    "# -----------------------------------\n",
    "def generate_amr_predictions(model, tokenizer, test_dataset, max_samples=100):\n",
    "    \"\"\"Generates AMR graphs from input sentences using the fine-tuned model.\"\"\"\n",
    "    predictions = []\n",
    "\n",
    "    for i, example in enumerate(test_dataset):\n",
    "        if i >= max_samples:  # Limit number of samples for efficiency\n",
    "            break\n",
    "\n",
    "        input_text = \"parse: \" + example[\"sentence\"]\n",
    "        input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(input_ids, max_length=512)\n",
    "        \n",
    "        predicted_amr = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        predicted_amr = fix_duplicate_nodes(predicted_amr)  # ✅ Ensure AMR is valid\n",
    "\n",
    "        gold_amr = fix_duplicate_nodes(example[\"amr\"])  # ✅ Fix duplicates in gold AMRs too\n",
    "\n",
    "        predictions.append((gold_amr, predicted_amr))\n",
    "\n",
    "    return predictions\n",
    "\n",
    "# Generate predictions\n",
    "predictions = generate_amr_predictions(model, tokenizer, test_dataset)\n",
    "\n",
    "# -----------------------------------\n",
    "# ⿥ Compute Smatch Score (with Fixes)\n",
    "# -----------------------------------\n",
    "def is_valid_amr(amr):\n",
    "    \"\"\"Checks if AMR is valid by ensuring it has at least one '/' symbol.\"\"\"\n",
    "    return amr.count('/') > 0\n",
    "\n",
    "def compute_smatch(gold_amrs, predicted_amrs):\n",
    "    \"\"\"Computes the Smatch score while handling errors.\"\"\"\n",
    "    total_precision, total_recall, total_f1 = 0, 0, 0\n",
    "    valid_samples = 0\n",
    "\n",
    "    for i, (gold, pred) in enumerate(zip(gold_amrs, predicted_amrs)):\n",
    "        try:\n",
    "            # ✅ Ensure AMRs are valid\n",
    "            if not is_valid_amr(pred) or not is_valid_amr(gold):\n",
    "                print(f\"⚠ Skipping sample {i} due to invalid AMR\")\n",
    "                continue\n",
    "\n",
    "            precision, recall, f_score = smatch.get_amr_match(str(gold), str(pred))\n",
    "            total_precision += precision\n",
    "            total_recall += recall\n",
    "            total_f1 += f_score\n",
    "            valid_samples += 1\n",
    "        except Exception as e:\n",
    "            print(f\"⚠ Error in Smatch calculation for sample {i}: {e}\")\n",
    "            print(f\"Gold AMR: {gold}\")\n",
    "            print(f\"Predicted AMR: {pred}\")\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "    if valid_samples == 0:\n",
    "        return 0, 0, 0  # Avoid division by zero\n",
    "\n",
    "    return total_precision / valid_samples, total_recall / valid_samples, total_f1 / valid_samples\n",
    "\n",
    "# Extract predicted AMRs for Smatch evaluation\n",
    "gold_amrs = [gold for gold, _ in predictions if gold is not None]\n",
    "predicted_amrs = [pred for _, pred in predictions if pred is not None]\n",
    "\n",
    "# Compute Smatch Score\n",
    "precision, recall, f1 = compute_smatch(gold_amrs, predicted_amrs)\n",
    "\n",
    "# -----------------------------------\n",
    "# ⿦ Display Results\n",
    "# -----------------------------------\n",
    "print(f\"🔥 Smatch Score - Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
